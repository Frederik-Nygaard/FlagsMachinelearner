Please see the attatched pdf for an easier to read version


1  IntroductionFlags are undoubtedly associated with culture all over the world, but is a flagindicative of more than what people have thought was pretty back when it wasdesigned?By examining categorical features for the design of the flags of the world, pro-vided by the university of California, we will attempt to predict the languagespoken a new country, based solely on the features of its flag.1.1  Basis for the ideaWhile flags may seem a rather strange data basis, there are some obvious indi-cations in flags, that describes what country they belong to.  An example of suchfeatures is the well known scandinavian ”cross flags”.  These allbelong to christian countries.  In the same way, flags in the islamic world oftencontains the Islamic symbol- the half-moon.I wish to explore whether or not this principle is prevalent enough to predictnot only the religion of a given country, but also to ”core language” spoken inthat country.1.2  About the dataThe  dataset  is  provided  from  the  University  of  California.   In  the  program,the data should be downloaded automatically, if not already existing.  In casesomething fails in this process, please manually dowload the data from https://archive.ics.uci.edu/ml/machine-learning-databases/flags/flag.dataandsave it in the ”data” folder.Naturally, normal machine learning would require far more data for each fea-ture, in order to create accurate predictions.  In this case however, there is nomore data to be had, as the number of flags in the world is rather limited.  Thiswill mean, that the accuracy of the predictions must be expected to be low.As the data is only categorical data, this will provide some further challenges,that is explained bellow.
2  MethodsIn  order  to  get  decent  results  there  is  a  number  of  steps  that  must  be  takenbeforehand.  The first of these is data preparation.  In order to avoid confusion,and minimize the run time of the actual machine learning program, this is donein a separate file, and does only need to be computed once.Once the data is prepared, the actual partition can be made.  This is done bythe standard data partition tools in the carret package.Hereafter the actual models can be created.  The aim is to explore the accuracyof different models, and compare these.  Specifically, the training data should beapplied to KNN, LDA, and decision tree- generating algorithms, and comparedto the accuracy of pure guessing.  Finally, the results will be examined, and aconclusion will be drawn.2.1  Code Comments2.1.1  Data PrepStarting in the ”Data Prep” file,  the data is downloaded and loaded into theprogram in lines 12-18, where after headers is added manually to the data frame,for better readability, as well as converting the data to a data frame.  This pro-cess is done on 19-33.although the fact that data is presented as numerical values might be deceiving,all data provided is categorical.  To avoid future confusion, the original valuesare replaced with their corresponding category names.  the information on this isavailable fromhttps://archive.ics.uci.edu/ml/machine-learning-databases/flags/flag.names.  This is done on lines 35-46.
For ease of reading, a new vector, namesProcessed, is created on line 49-56, itsmain  use  is  to  check  what  columns  has  already  been  processed,  and  as  suchshould be ignored unless for debugging.Some of the features, such as number of stripes in the flag have been groupedin the original data, while others, such as shapes present in the flag, is spreadout.  In order to achieve unison, all the grouped features are split out, so thatthe values are essentially boolean features of ”is present” for all features.  Thesemodifications are done in lines 57-78.  The column names are adjusted in lines79-99.Finally, the processed data is saved separately in the document, on lines 100-104.2.1.2  MachinelearnerPlease make sure that the correct packages are installed on your sytsem, as theprogram will not run otherwise.Initially the data is loaded in, and a data partition is made using carets ”cre-atedatapartition” function.  in order to ensure a decent size of the test set, thepartition is set to be 20% of the available data.From a visualization of the data, it was apparent that some groups of languageswas too infrequent to yield usable results.  As such, these were grouped together,so that we will be predicting the languages based on 6 groups, instead of theinitial 10 language groups.  This correction is made on lines 29-42.Once the data is finally in a working order, the predictions can finally start.In order to have a baseline for future comparissions, a ”pure guess” attempt ismade.  This is done using a simple sample function, which is then compared tothe actual languages spoken.  Based on this particular seed, not a single success-ful guess was made.  This can be an indication that the languages is not evenlydistributed.  Clearly there is room for improvement.For all the analysis below, the method is the same, and will be described as one:the model is trained through the corresponding caret package, after which theconfusion matrix is generated for the trained model.  The accuracy of the modelis then extracted and stored for future comparison.
3  Discussion and conclusionThe program attempts 3 different approaches to finding a decent way of pre-dicting the language spoken in countries based on the features of the country’sflag.Initially, it is clear that an approach of pure guessing is not feasible; unlike the”heights” data set that has been presented in class, it is not necessary to accountfor uneven distribution compared to an eventual new data input, as this is the”true” distribution of languages.A k-nearest neighbour approach seem to have a decent accuracy,  however wenotice that when an attempt is made to make a more optimize knn, by tryingdifferent values of K, the accuracy drops.  This could be indicitative of a codingmistake.personally,  i  had  expected  a  decision  tree  to  be  a  well  performing  model,  asthese should be handling categorical data rather well.  This did however provenot to be the case, with the worst accuracy of all models.An LDA seems to have been the best approach,  with an accuracy of approx.44%.  While this is not high in terms of normal machine learning models, this isdone on a small data set, and is still greatly outperforming attempts to purelyguess the language.  As such this must be considered an successful model, andindicates, that there is a connection between the languages flags of the world